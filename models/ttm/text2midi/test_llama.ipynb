{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig, LlamaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(256, 1024, padding_idx=250)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=256, bias=False)\n",
      ")\n",
      "Number of parameters: 201.88M\n"
     ]
    }
   ],
   "source": [
    "llama_config = LlamaConfig(vocab_size=256,\n",
    "                           hidden_size=1024,\n",
    "                           intermediate_size=4096,\n",
    "                           num_hidden_layers=12,\n",
    "                           num_attention_heads=16,\n",
    "                           pad_token_id=250,\n",
    "                           bos_token_id=251,\n",
    "                           eos_token_id=252,)\n",
    "# save config\n",
    "llama_config.save_pretrained('/home/t-zeqianju/yuancwang/AmphionOpen/data/llama_config')\n",
    "\n",
    "llama_model = LlamaForCausalLM(llama_config)\n",
    "print(llama_model)\n",
    "# print number of parameters\n",
    "print(f\"Number of parameters: {llama_model.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[251, 1, 4, 5, 6, 3, 252], [251, 1, 2, 3, 252, 250, 250], [251, 4, 5, 252, 250, 250, 250], [251, 6, 7, 8, 9, 10, 252], [251, 11, 12, 13, 14, 252, 250]]\n",
      "[[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0]]\n",
      "[[251, 1, 4, 5, 6, 3, 252], [251, 1, 2, 3, 252, -100, -100], [251, 4, 5, 252, -100, -100, -100], [251, 6, 7, 8, 9, 10, 252], [251, 11, 12, 13, 14, 252, -100]]\n",
      "torch.Size([5, 7])\n",
      "torch.Size([5, 7])\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "def add_padding(input_ids, pad_token_id, max_length=None):\n",
    "    # input_ids: List[List[int]]\n",
    "    # pad_token_id: int\n",
    "    # max_length: int\n",
    "    if max_length is None:\n",
    "        max_length = max(len(ids) for ids in input_ids)\n",
    "    padded_input_ids = []\n",
    "    for ids in input_ids:\n",
    "        padded_ids = ids + [pad_token_id] * (max_length - len(ids))\n",
    "        padded_input_ids.append(padded_ids)\n",
    "    return padded_input_ids\n",
    "\n",
    "def add_bos(input_ids, bos_token_id):\n",
    "    # input_ids: List[List[int]]\n",
    "    # bos_token_id: int\n",
    "    bos_input_ids = [[bos_token_id] + ids for ids in input_ids]\n",
    "    return bos_input_ids\n",
    "\n",
    "def add_eos(input_ids, eos_token_id):\n",
    "    # input_ids: List[List[int]]\n",
    "    # eos_token_id: int\n",
    "    eos_input_ids = [ids + [eos_token_id] for ids in input_ids]\n",
    "    return eos_input_ids\n",
    "\n",
    "def add_mask(input_ids, pad_token_id):\n",
    "    # input_ids: List[List[int]]\n",
    "    # pad_token_id: int\n",
    "    attention_mask = [[int(token_id != pad_token_id) for token_id in ids] for ids in input_ids]\n",
    "    return attention_mask\n",
    "\n",
    "def add_labels(input_ids, pad_token_id):\n",
    "    # input_ids: List[List[int]]\n",
    "    # pad_token_id: int\n",
    "    labels = [[token_id if token_id != pad_token_id else -100 for token_id in ids] for ids in input_ids]\n",
    "    return labels\n",
    "\n",
    "input_token_ids = [[1, 4, 5, 6, 3], [1, 2, 3], [4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14]]\n",
    "\n",
    "input_token_ids = add_bos(input_token_ids, llama_config.bos_token_id)\n",
    "input_token_ids = add_eos(input_token_ids, llama_config.eos_token_id)\n",
    "input_token_ids = add_padding(input_token_ids, llama_config.pad_token_id)\n",
    "attention_mask = add_mask(input_token_ids, llama_config.pad_token_id)\n",
    "labels = add_labels(input_token_ids, llama_config.pad_token_id)\n",
    "\n",
    "print(input_token_ids)\n",
    "print(attention_mask)\n",
    "print(labels)\n",
    "\n",
    "input_token_ids = torch.tensor(input_token_ids)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(input_token_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.326286792755127\n",
      "torch.Size([5, 7, 256])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "llama_model.to(device)\n",
    "input_token_ids = input_token_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = llama_model(input_token_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "print(out.loss.item())\n",
    "print(out.logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
